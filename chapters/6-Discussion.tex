\chapter{Discussion}
This chapter discusses the work done in this thesis, both what was done good which lead to new knowledge and contributions, but also what could have been done differently. Challenges and mistakes which lead to decisions being made are explained here. Limitations to this research will also be included as this could have affected how strong the results are and can explain why certain decisions were made. 

When starting this research, a decision to include several different air quality monitors from different manufactures were made. Looking at the results and evaluation, this was positive for the contribution as it shows that there are differences in level of private information inference from the different devices. This shows that analyzing the network traffic from one air quality monitor is not representative for all air quality monitors and is something to consider if setting up a test environment where only one air quality monitor is included.

Originally, only the routine test cases were selected, cooking, showering and window open. For the work of setting up the devices, sniffer and how to capture, traffic was captured over a longer period of time to also test the storage and processing power of the hardware used. During this initial setup, a discovery of the differences in traffic for Netatmo Smart Indoor Air Quality Monitor from when the user was gone or not were very visible. Therefore, the fourth test case was defined and tested on all the devices. 

When defining the scope and test cases, it was unclear how the devices communicated including packet sizes, amount of packets, how frequent and if traffic was encrypted or not. When starting the capturing and analyzing of data from the different devices, it became clear that the devices do not send the same amount of traffic. Netatmo Smart Indoor Air Quality Monitor sends the least amount of packets, while Mill Sense and Nedis SmartLife sends significantly more packets. For Nedis SmartLife, analyzing the amount of traffic was a challenge, both time-wise as creating graphs took a long time, but also looking more into the traffic. The processing power of a standard computer were just enough to process the data and figures shown in the evaluation and analysis result chapter. 

Many decisions and limitations had to be set for this research because of time constraints. Originally the plan was to test more air quality monitors from other manufactures, but also communicating on different communication protocols, such as Bluetooth or ZigBee. However, seeing the amount of work and results that needed to be analyzed to do this in a good way, only \gls{Wi-Fi} devices were selected. This part is therefore moved to future work.  

A challenge faced during the research happened for the test cases for cooking. The events were designed to change the indoor environment so much that the air quality monitors would sense values outside of the defined threshold values. However, when doing the cooking test, only a few of the executions actually triggered the notifications to be send. On the other side, for showering and window open, every execution lead to a significant change in threshold values and sent several notifications to the connected phone. 

Another challenge, which was encountered during the test, was the differences in the baseline capturings compared to the event traffic for Nedis SmartLife and Mill Sense, before the event was triggered, as we would have expected these two traffic patterns to be similar. However, the decision were made to include the baseline capturings and compare them to the event traffic because it also shows methodologically the setup that was originally designed, and it is still possible to compare traffic from when the event was ongoing to traffic before and after the event. It is unclear why there are differences in standard traffic from the devices, but differences in season as the test cases have been conducted during January and the baseline during March can be one reason. Another reason can be that the baseline was not captured at the exact same spot as were the test cases were carried out. This was because of time constraints. It was not enough time to have a long baseline in each room and conduct the test cases during the research period even though this would have been preferred. Since the air quality monitors are \gls{IoT} sensor that are always on and sense the environment at the time, differences day to day and month to month can be significant. The indoor environment can be affected by many different factors and it is impossible to have the exact same values for the indoor air during longer period of times. 

The differences in standard traffic shows that in order to have the same traffic pattern to test with, the best way would be to have the air quality monitors in a closed environment. Then the environment could only be changed by the specific event which are tested. However, this takes away the realistic parts of the test. The chance of identifying the specific events may be bigger, but if it is not applicable in a live environment, there is no use to launch such an attack against a target. Another aspect of this is that if changes in the environment is significant within the same environment then it will also be hard for an attacker to find certain signatures in their own environment to be applicable on a target environment and device. 